{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Set-Up\n",
    "\n",
    "To accomplish this task, I used a Windows machine with Pyspark set-up. And since it contains multiple questions, I would rely on Jupyter Notebook to quickly prototype a solution with some comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://victor-home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>victor_paytm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b6bed26988>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, lit, col, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"victor_paytm\").getOrCreate()\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "### 1. Load the global weather data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned =spark.read.option(\"header\", \"true\").csv('data/2019/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+-----+------+\n",
      "|   STN| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP| SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+-----+------+\n",
      "|010260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|29.8|0.02| 18.5|001000|\n",
      "|010260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1|27.1|0.48| 22.8|001000|\n",
      "|010260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4|37.4|0.25|999.9|011000|\n",
      "|010260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|36.1|0.52|999.9|001000|\n",
      "|010260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|38.5|38.5|0.02| 23.6|010000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only the first four characters from the column \n",
    "partitioned = partitioned.withColumn(\"PRCP\",expr(\"substring(PRCP, 1, length(PRCP)-1)\"))\n",
    "\n",
    "# there are characters in these columns\n",
    "partitioned = partitioned.withColumn(\"MAX\",partitioned.MAX.substr(1, 4))\n",
    "partitioned = partitioned.withColumn(\"MIN\",partitioned.MAX.substr(1, 4))\n",
    "\n",
    "# the column name is not appropriate\n",
    "partitioned =partitioned.withColumnRenamed(\"STN---\", \"STN\")\n",
    "partitioned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, I would handle the missing value in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(None))\n",
    "\n",
    "nullDict = {'TEMP': 9999.9,\n",
    "            'DEWP': 9999.9,\n",
    "            'SLP': 9999.9,\n",
    "            'STP': 9999.9,\n",
    "            'VISIB': 999.9,\n",
    "            'WDSP': 999.9,\n",
    "            'MXSPD': 999.9,\n",
    "            'GUST':999.9,\n",
    "            'MAX':9999.9,\n",
    "            'MIN':9999.9,\n",
    "            'PRCP':99.9,\n",
    "            'SNDP':999.9}\n",
    "# loop the dict to deal with NA\n",
    "for key, val in nullDict.items():\n",
    "    partitioned = partitioned.withColumn(key,replace(col(key), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n",
      "|   STN| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n",
      "|010260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|29.8|0.02|18.5|001000|\n",
      "|010260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1|27.1|0.48|22.8|001000|\n",
      "|010260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4|37.4|0.25|null|011000|\n",
      "|010260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|36.1|0.52|null|001000|\n",
      "|010260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|38.5|38.5|0.02|23.6|010000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partitioned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Join the stationlist.csv with the countrylist.csv to get the full country name for each station number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|012240|          NO|\n",
      "|020690|          SW|\n",
      "|020870|          SW|\n",
      "|021190|          SW|\n",
      "|032690|          UK|\n",
      "+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationlist = spark.read.option(\"header\", \"true\").csv('stationlist.csv')\n",
    "stationlist.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|COUNTRY_ABBR|       COUNTRY_FULL|\n",
      "+------------+-------------------+\n",
      "|          AA|              ARUBA|\n",
      "|          AC|ANTIGUA AND BARBUDA|\n",
      "|          AF|        AFGHANISTAN|\n",
      "|          AG|            ALGERIA|\n",
      "|          AI|   ASCENSION ISLAND|\n",
      "+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countrylist = spark.read.option(\"header\", \"true\").csv('countrylist.csv')\n",
    "countrylist.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationWithCountry = stationlist.join(countrylist,stationlist.COUNTRY_ABBR ==  countrylist.COUNTRY_ABBR,\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Join the global weather data with the full country names by station number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------+------------+------------+------------+\n",
      "|   STN| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|STN_NO|COUNTRY_ABBR|COUNTRY_ABBR|COUNTRY_FULL|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------+------------+------------+------------+\n",
      "|010260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|29.8|0.02|18.5|001000|010260|          NO|          NO|      NORWAY|\n",
      "|010260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1|27.1|0.48|22.8|001000|010260|          NO|          NO|      NORWAY|\n",
      "|010260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4|37.4|0.25|null|011000|010260|          NO|          NO|      NORWAY|\n",
      "|010260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|36.1|0.52|null|001000|010260|          NO|          NO|      NORWAY|\n",
      "|010260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|38.5|38.5|0.02|23.6|010000|010260|          NO|          NO|      NORWAY|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "globWeatherCountry = partitioned.join(stationWithCountry, partitioned.STN == stationWithCountry.STN_NO,\"left\")\n",
    "globWeatherCountry.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "### 1. Which country had the hottest average mean temperature over the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|COUNTRY_FULL|    avg_mean_temp|\n",
      "+------------+-----------------+\n",
      "|    DJIBOUTI|89.56927710843374|\n",
      "|        CHAD|  86.908357771261|\n",
      "|       NIGER|84.61118216700164|\n",
      "|       SUDAN|84.01162790697674|\n",
      "| EL SALVADOR| 83.9929676511955|\n",
      "+------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "# change type to integer\n",
    "\n",
    "globWeatherCountry.withColumn(\"TEMP\", globWeatherCountry[\"TEMP\"].cast(IntegerType()))\\\n",
    ".groupBy(\"COUNTRY_FULL\").avg(\"TEMP\").\\\n",
    "withColumnRenamed(\"avg(TEMP)\", \"avg_mean_temp\").\\\n",
    "sort(\"avg_mean_temp\",ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, DJIBOUTI had the hottest average mean temperature over the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which country had the most consecutive days of tornadoes/funnel cloud formations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------------+\n",
      "|COUNTRY_FULL|YEARMODA|max_tornado_by_country_day|\n",
      "+------------+--------+--------------------------+\n",
      "|    ZIMBABWE|20191231|                         0|\n",
      "|    ZIMBABWE|20191230|                         0|\n",
      "|    ZIMBABWE|20191229|                         0|\n",
      "|    ZIMBABWE|20191228|                         0|\n",
      "|    ZIMBABWE|20191227|                         0|\n",
      "+------------+--------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tornado_df = globWeatherCountry.\\\n",
    "select('COUNTRY_FULL','YEARMODA','FRSHTT').\\\n",
    "withColumn('Tornado',partitioned.FRSHTT.substr(6,7))\n",
    "tornado_df = tornado_df.withColumn(\"Tornado\", tornado_df[\"Tornado\"].cast(IntegerType()))\n",
    "\n",
    "\n",
    "tornado_df = tornado_df.groupBy(['COUNTRY_FULL','YEARMODA']).agg({'Tornado':'max'}).\\\n",
    "withColumnRenamed(\"max(Tornado)\",\"max_tornado_by_country_day\").\\\n",
    "sort([\"COUNTRY_FULL\",'YEARMODA'],ascending=False)\n",
    "tornado_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To solve this problem, I used a self-defined UDF to aggregate all data\n",
    "def mostConsecutive(seq):\n",
    "    globalMax = 0\n",
    "    localMax = 0\n",
    "    for fast in range(len(seq)):\n",
    "        if seq[fast] == 1:\n",
    "            localMax += 1\n",
    "            globalMax = max(globalMax, localMax)\n",
    "        else:\n",
    "            localMax = 0\n",
    "    return globalMax\n",
    "\n",
    "\n",
    "# doing a small test\n",
    "seq = [0,0,1,1,1,1,1,0,0,1,1,1,0]\n",
    "mostConsecutive(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|  COUNTRY_FULL|num_tornadoes|\n",
      "+--------------+-------------+\n",
      "|         GHANA|            2|\n",
      "|CAYMAN ISLANDS|            2|\n",
      "|         ITALY|            2|\n",
      "|         JAPAN|            2|\n",
      "|        CANADA|            2|\n",
      "|         INDIA|            2|\n",
      "| UNITED STATES|            2|\n",
      "|      MALDIVES|            1|\n",
      "|       SENEGAL|            1|\n",
      "|        JERSEY|            1|\n",
      "+--------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfMostConsecutive = F.udf(mostConsecutive, IntegerType())\n",
    "tornado_df.\\\n",
    "groupBy('COUNTRY_FULL').\\\n",
    "agg(udfMostConsecutive(F.collect_list('max_tornado_by_country_day')).alias('num_tornadoes')).\\\n",
    "sort(\"num_tornadoes\",ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, there are seven countries had the same consecutive Tornadoes days. Which are:\n",
    "1. India, 2 days\n",
    "2. Japan, 2 days\n",
    "3. Canada, 2 days\n",
    "4. Cayman Islands, 2 days\n",
    "5. Italy, 2 days\n",
    "6. Ghana, 2 days\n",
    "7. United States, 2 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which country had the second highest average mean wind speed over the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|        COUNTRY_FULL|avg_mean_wind_speed|\n",
      "+--------------------+-------------------+\n",
      "|FALKLAND ISLANDS ...| 17.429920477137177|\n",
      "|               ARUBA| 15.513661202185792|\n",
      "|       FAROE ISLANDS| 14.845360824742269|\n",
      "|FRENCH SOUTHERN A...| 13.769833496571989|\n",
      "|            BARBADOS| 13.653005464480874|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "globWeatherCountry.withColumn(\"WDSP\", globWeatherCountry[\"WDSP\"].cast(IntegerType())).\\\n",
    "groupBy(\"COUNTRY_FULL\").\\\n",
    "avg(\"WDSP\").\\\n",
    "withColumnRenamed(\"avg(WDSP)\", \"avg_mean_wind_speed\").\\\n",
    "sort(\"avg_mean_wind_speed\",ascending=False).\\\n",
    "show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second higher average mean wind speed is Aruba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
